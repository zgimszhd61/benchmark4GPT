Human: I'm going to give you a document. Then I'm going to ask you a question about
it. I'd like you to first write down exact quotes of parts of the document that would
help answer the question, and then I'd like you to answer the question using facts
from the quoted content. Here is the document:
<document>
Anthropic: Challenges in evaluating AI systems
Introduction
Most conversations around the societal impacts of artificial intelligence (AI) come
down to discussing some quality of an AI system, such as its truthfulness, fairness,
potential for misuse, and so on. We are able to talk about these characteristics
because we can technically evaluate models for their performance in these areas. what many people working inside and outside of AI don’t fully appreciate is how
difficult it is to build robust and reliable model evaluations. Many of today’s
existing evaluation suites are limited in their ability to serve as accurate
indicators of model capabilities or safety.
At Anthropic, we spend a lot of time building evaluations to better understand our AI
systems. We also use evaluations to improve our safety as an organization, as
illustrated by our Responsible Scaling Policy. In doing so, we have grown to
appreciate some of the ways in which developing and running evaluations can be
challenging.
